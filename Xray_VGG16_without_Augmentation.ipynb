{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First Attempt at using transfer learning with the VGG16 Model: Without Data Augmentation"
      ],
      "metadata": {
        "id": "rDSW2MngxfjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoAXAnaQ0A3v",
        "outputId": "88b694a6-0328-4174-960c-47f1fb2451d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Train and Test Data\n"
      ],
      "metadata": {
        "id": "aFgVjRmhwLpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize for grayscale X-rays\n",
        "])\n",
        "\n",
        "# Load dataset (assumes dataset is in a directory with subfolders for each class)\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/datasets/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/datasets/test', transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers= 4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Check class to index mapping\n",
        "print(train_dataset.class_to_idx)  # {'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}"
      ],
      "metadata": {
        "id": "uJh0Kt5p7lXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load Pretrained VGG16 model"
      ],
      "metadata": {
        "id": "Wa3EfRzOwXlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "# Load a pretrained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Modify the classifier (for our specific number of classes, e.g., 2 classes for binary classification)\n",
        "model.classifier[6] = nn.Linear(4096, 3)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "8W3hJZXbC55F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. First Round of Training: 10 epochs"
      ],
      "metadata": {
        "id": "MzSCuH-owg5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (you can use Adam or SGD)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "print('Training finished.')\n",
        "\n",
        "torch.save(model.state_dict(), 'VGG16_on_xray_weights.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie7Vi6yVESLP",
        "outputId": "e4ad8f81-9dd0-47cf-ce5c-2272c15f7872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5405989069880152\n",
            "Epoch [2/10], Loss: 0.3995693463123649\n",
            "Epoch [3/10], Loss: 0.34103925370731236\n",
            "Epoch [4/10], Loss: 0.2710988822798788\n",
            "Epoch [5/10], Loss: 0.19787369323380155\n",
            "Epoch [6/10], Loss: 0.14987668337906065\n",
            "Epoch [7/10], Loss: 0.11559754992303856\n",
            "Epoch [8/10], Loss: 0.09030536668192297\n",
            "Epoch [9/10], Loss: 0.07235099921900025\n",
            "Epoch [10/10], Loss: 0.05164470511115087\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate Accuracy"
      ],
      "metadata": {
        "id": "pItA_XTIwsmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Function to evaluate the model on the test dataset\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            # Move images and labels to the device (GPU)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Example usage after training\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "3WiMCghmE5XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e88965-081a-4e1d-b13c-2404f7bf0250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 66.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Attempt to Improve Accuracy by:\n",
        "    - training an additional 20 epochs\n",
        "    - using a learning rate scheduler"
      ],
      "metadata": {
        "id": "14DDJZkxw0z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Load the saved model state to resume training\n",
        "model.load_state_dict(torch.load('VGG16_on_xray_weights.pth', weights_only = True))\n",
        "\n",
        "# Freeze all layers except the classifier (if not already done)\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Continue training for another 20 epochs\n",
        "num_epochs = 20  # Adjust the number of additional epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n",
        "\n",
        "    # Step the scheduler after each epoch, based on the average loss\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "print('Additional training finished.')\n",
        "\n",
        "# Save the updated model state after the additional 20 epochs\n",
        "torch.save(model.state_dict(), 'VGG16_on_xray_weights_updated.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99nBseTL-B9A",
        "outputId": "adc9fc77-bdbc-41ae-d9c0-573682565bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.06268801990799332\n",
            "Epoch [2/20], Loss: 0.0755457083006858\n",
            "Epoch [3/20], Loss: 0.05614433191828087\n",
            "Epoch [4/20], Loss: 0.054475882787423845\n",
            "Epoch [5/20], Loss: 0.028324159564474666\n",
            "Epoch [6/20], Loss: 0.04040302057747666\n",
            "Epoch [7/20], Loss: 0.033309879123921214\n",
            "Epoch [8/20], Loss: 0.049224281979334095\n",
            "Epoch [9/20], Loss: 0.03608592369168862\n",
            "Epoch [10/20], Loss: 0.020256091257685117\n",
            "Epoch [11/20], Loss: 0.005477504802788672\n",
            "Epoch [12/20], Loss: 0.006953000420253375\n",
            "Epoch [13/20], Loss: 0.00325476971110404\n",
            "Epoch [14/20], Loss: 0.0024957552925110454\n",
            "Epoch [15/20], Loss: 0.0023376578120333073\n",
            "Epoch [16/20], Loss: 0.0025249634888462598\n",
            "Epoch [17/20], Loss: 0.002905294106533891\n",
            "Epoch [18/20], Loss: 0.0019604945106901006\n",
            "Epoch [19/20], Loss: 0.0011480812233954577\n",
            "Epoch [20/20], Loss: 0.0009419707023567109\n",
            "Additional training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Evaluate Model Once More"
      ],
      "metadata": {
        "id": "n22sE-3YxIyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model once more\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr9by-LbI50r",
        "outputId": "9d7dc2d2-8739-4a99-8aa6-f9c323db0194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the learning rate dropped significantly, without seeing an equivalent increase in accuracy, I stopped training here and looked into other methods to improve accuracy. Data Augmentation."
      ],
      "metadata": {
        "id": "mUXd-ETCxPl8"
      }
    }
  ]
}