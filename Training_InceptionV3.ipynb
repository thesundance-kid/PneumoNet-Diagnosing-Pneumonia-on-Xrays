{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pediatric Pneumonia Detection using InceptionV3\n",
        "\n",
        "## Overview\n",
        "This notebook implements transfer learning with InceptionV3 for automated detection and classification of pneumonia from pediatric chest X-rays.\n",
        "\n",
        "### Model Architecture\n",
        "- Base: InceptionV3 pretrained on ImageNet\n",
        "- Modified layers: Mixed_7b, Mixed_7c, and final FC\n",
        "- Output classes: 3 (Normal, Bacterial Pneumonia, Viral Pneumonia)\n",
        "\n",
        "### Training Configuration\n",
        "- Learning rate: 0.0001\n",
        "- Batch size: 32\n",
        "- Early stopping patience: 3\n",
        "- Data augmentation: light rotation, translation, scaling\n",
        "\n",
        "### Environment Requirements\n",
        "- Python 3.10\n",
        "- PyTorch 2.1.0\n",
        "- torchvision 0.16.0\n",
        "- CUDA compatible GPU\n",
        "\n",
        "### Dataset\n",
        "Uses the Chest X-Ray Images (Pneumonia) dataset containing:\n",
        "- 5,856 validated chest X-ray images\n",
        "- Age range: 1-5 years\n",
        "- Categories: Normal, Bacterial Pneumonia, Viral Pneumonia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Mount Google Drive for accessing the dataset and saving model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZiIMQ_jF1q1",
        "outputId": "e7acfaf1-313d-4e32-ce28-8ce585fcb664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy training and testing data from Google Drive to local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILKzLrb5Gwyw"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/datasets/train /content/train\n",
        "!cp -r /content/drive/MyDrive/datasets/test /content/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Exploration\n",
        "\n",
        "Analyze dataset distribution and create training/validation split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87R-CwwmHMpL",
        "outputId": "f0640a3d-7ce0-4e50-8904-0b5a4283f8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples in viral pneumonia: 1345\n",
            "Number of examples in bacterial pneumonia: 2530\n",
            "Number of examples in a_normal_xrays: 1341\n",
            "Number of examples in viral pneumonia: 148\n",
            "Number of examples in bacterial pneumonia: 242\n",
            "Number of examples in a_normal_xrays: 234\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the paths to train and test directories\n",
        "train_dir = '/content/train'\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# Count number of examples per class in the training set\n",
        "for class_dir in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_dir)\n",
        "    if os.path.isdir(class_path):\n",
        "        num_examples = len(os.listdir(class_path))\n",
        "        print(f'Number of examples in {class_dir}: {num_examples}')\n",
        "\n",
        "# Count number of examples per class in the test set\n",
        "for class_dir in os.listdir(test_dir):\n",
        "    class_path = os.path.join(test_dir, class_dir)\n",
        "    if os.path.isdir(class_path):\n",
        "        num_examples = len(os.listdir(class_path))\n",
        "        print(f'Number of examples in {class_dir}: {num_examples}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create training/validation split (90/10 percent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esAP_mw6J96g",
        "outputId": "5d8681a0-9346-4f55-abe0-03574f430144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 4694\n",
            "Validation set size: 522\n"
          ]
        }
      ],
      "source": [
        "#Create Validation Set using 10% of train set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare empty lists to hold file paths and labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Assign integer labels for each class (assuming three classes)\n",
        "class_map = {'viral pneumonia': 0, 'bacterial pneumonia': 1, 'a_normal_xrays': 2}\n",
        "\n",
        "# Iterate through each class folder and gather file paths and labels\n",
        "for class_name, class_label in class_map.items():\n",
        "    class_folder = os.path.join(train_dir, class_name)\n",
        "    for file_name in os.listdir(class_folder):\n",
        "        file_path = os.path.join(class_folder, file_name)\n",
        "        file_paths.append(file_path)\n",
        "        labels.append(class_label)\n",
        "\n",
        "# Split the data (10% for validation, 90% for training)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    file_paths, labels, test_size=0.1, random_state=42, stratify=labels)\n",
        "\n",
        "# Check the number of images\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMNocQOtMBEd",
        "outputId": "8255d1f6-dffb-4fe3-ff01-c3da623fd00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Define the path to your datasets folder in Google Drive\n",
        "validation_folder = '/content/drive/MyDrive/datasets/validation'\n",
        "\n",
        "# Create subdirectories for each class in the validation folder\n",
        "for class_name in class_map.keys():\n",
        "    os.makedirs(os.path.join(validation_folder, class_name), exist_ok=True)\n",
        "\n",
        "# Move validation files to the corresponding class folder in the validation folder\n",
        "for file_path, label in zip(X_val, y_val):\n",
        "    # Get the class name corresponding to the label\n",
        "    class_name = list(class_map.keys())[list(class_map.values()).index(label)]\n",
        "\n",
        "    # Define the destination path in the validation folder\n",
        "    destination_path = os.path.join(validation_folder, class_name, os.path.basename(file_path))\n",
        "\n",
        "    # Copy the file to the validation folder in Google Drive\n",
        "    shutil.copy(file_path, destination_path)\n",
        "\n",
        "print(\"Validation set copied to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H6aNwkxMdk7"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/datasets/validation /content/validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing and Loading\n",
        "\n",
        "Calculate dataset statistics for normalization and prepare data transforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvHoxwlTM6iV",
        "outputId": "bda2c41d-cb87-4e22-dfac-376de8943879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean: tensor([0.4823, 0.4823, 0.4823]), Std: tensor([0.2220, 0.2220, 0.2220])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initial transform: Resize and convert to tensor only\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Ensure all images are the same size\n",
        "    transforms.ToTensor()  # Convert images to tensor format\n",
        "])\n",
        "\n",
        "# Load dataset with simple transform (no normalization)\n",
        "train_dataset_simple = datasets.ImageFolder(root='/content/train', transform=simple_transform)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_loader_simple = DataLoader(train_dataset_simple, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "def compute_mean_std(loader):\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    total_images_count = 0\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0)  # Number of images in the batch\n",
        "        images = images.view(batch_samples, images.size(1), -1)  # Flatten the images\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        total_images_count += batch_samples\n",
        "\n",
        "    mean /= total_images_count\n",
        "    std /= total_images_count\n",
        "    return mean, std\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean, std = compute_mean_std(train_loader_simple)\n",
        "\n",
        "print(f\"Mean: {mean}, Std: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV326JzaNoNi"
      },
      "source": [
        "Load and Normalize datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYx0svncNnYq",
        "outputId": "32ab646c-1e32-453e-b3ca-59ff7b4538ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n",
            "{'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n",
            "{'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n"
          ]
        }
      ],
      "source": [
        "# Use the computed mean and std values and perform data augmentation on train dataset\n",
        "train_transform = transforms.Compose([\n",
        "       transforms.RandomRotation(10),  # Small rotations for generalization\n",
        "       transforms.RandomAffine(degrees=0, translate=(0.03, 0.03)),  # Small translations\n",
        "       transforms.RandomResizedCrop(299, scale=(0.95, 1.05)),  # Slight zoom\n",
        "       transforms.ToTensor(),  # Convert to tensor\n",
        "       transforms.Normalize(mean=mean.tolist(), std=std.tolist())  # Use computed mean and std\n",
        "   ])\n",
        "\n",
        "#Only resize and normalize test dataset\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())  # Same normalization for test set\n",
        "])\n",
        "\n",
        "\n",
        "# Load dataset (assumes dataset is in a directory with subfolders for each class)\n",
        "train_dataset = datasets.ImageFolder(root='/content/train', transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root='/content/validation', transform=test_transform)\n",
        "test_dataset = datasets.ImageFolder(root='/content/test', transform=test_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers= 2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Check class to index mapping\n",
        "print(train_dataset.class_to_idx)  # {'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n",
        "print(val_dataset.class_to_idx)  # {'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n",
        "print(test_dataset.class_to_idx)  # {'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Architecture\n",
        "\n",
        "Initialize InceptionV3 model with transfer learning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zBpskU1OGsC",
        "outputId": "148b7ca2-96a2-4989-b9d9-b40aa1c2180b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 137MB/s] \n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load  pretrained InceptionV3 model\n",
        "model = models.inception_v3(pretrained=True)\n",
        "\n",
        "# Freeze the pre-trained layers (except the last fully connected layer)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer for 3 classes (pneumonia types)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 3)  # 3 classes: normal, bacterial pneumonia, viral pneumonia\n",
        "\n",
        "# Ensure the final layer parameters are trainable\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Configuration\n",
        "\n",
        "Configure training parameters:\n",
        "- Loss function: Cross Entropy\n",
        "- Optimizer: Adam\n",
        "- Learning rate scheduling (starting with .0001)\n",
        "- Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epgKMznCPvxn",
        "outputId": "05d904d5-3a94-495a-e962-38b7ec2bc3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20] - Current Learning Rate: 0.0001\n",
            "Epoch [1/20], Loss: 0.7768, Training Accuracy: 68.25%\n",
            "Validation Loss: 0.8052, Validation Accuracy: 69.92%\n",
            "Epoch [2/20] - Current Learning Rate: 0.0001\n",
            "Epoch [2/20], Loss: 0.7284, Training Accuracy: 71.03%\n",
            "Validation Loss: 0.7705, Validation Accuracy: 71.46%\n",
            "Epoch [3/20] - Current Learning Rate: 0.0001\n",
            "Epoch [3/20], Loss: 0.7024, Training Accuracy: 71.91%\n",
            "Validation Loss: 0.7537, Validation Accuracy: 72.03%\n",
            "Epoch [4/20] - Current Learning Rate: 0.0001\n",
            "Epoch [4/20], Loss: 0.6943, Training Accuracy: 72.35%\n",
            "Validation Loss: 0.7516, Validation Accuracy: 71.46%\n",
            "Epoch [5/20] - Current Learning Rate: 0.0001\n",
            "Epoch [5/20], Loss: 0.6708, Training Accuracy: 72.51%\n",
            "Validation Loss: 0.7210, Validation Accuracy: 72.61%\n",
            "Epoch [6/20] - Current Learning Rate: 0.0001\n",
            "Epoch [6/20], Loss: 0.6560, Training Accuracy: 73.68%\n",
            "Validation Loss: 0.7248, Validation Accuracy: 71.84%\n",
            "Epoch [7/20] - Current Learning Rate: 0.0001\n",
            "Epoch [7/20], Loss: 0.6496, Training Accuracy: 73.93%\n",
            "Validation Loss: 0.7056, Validation Accuracy: 72.80%\n",
            "Epoch [8/20] - Current Learning Rate: 0.0001\n",
            "Epoch [8/20], Loss: 0.6376, Training Accuracy: 73.89%\n",
            "Validation Loss: 0.6985, Validation Accuracy: 73.56%\n",
            "Epoch [9/20] - Current Learning Rate: 0.0001\n",
            "Epoch [9/20], Loss: 0.6292, Training Accuracy: 73.75%\n",
            "Validation Loss: 0.7012, Validation Accuracy: 72.61%\n",
            "Epoch [10/20] - Current Learning Rate: 0.0001\n",
            "Epoch [10/20], Loss: 0.6284, Training Accuracy: 74.44%\n",
            "Validation Loss: 0.6834, Validation Accuracy: 73.75%\n",
            "Epoch [11/20] - Current Learning Rate: 0.0001\n",
            "Epoch [11/20], Loss: 0.6300, Training Accuracy: 73.81%\n",
            "Validation Loss: 0.6812, Validation Accuracy: 73.18%\n",
            "Epoch [12/20] - Current Learning Rate: 0.0001\n",
            "Epoch [12/20], Loss: 0.6226, Training Accuracy: 74.29%\n",
            "Validation Loss: 0.6762, Validation Accuracy: 73.95%\n",
            "Epoch [13/20] - Current Learning Rate: 0.0001\n",
            "Epoch [13/20], Loss: 0.6159, Training Accuracy: 74.08%\n",
            "Validation Loss: 0.6891, Validation Accuracy: 73.56%\n",
            "Epoch [14/20] - Current Learning Rate: 0.0001\n",
            "Epoch [14/20], Loss: 0.6163, Training Accuracy: 73.77%\n",
            "Validation Loss: 0.6716, Validation Accuracy: 74.52%\n",
            "Epoch [15/20] - Current Learning Rate: 0.0001\n",
            "Epoch [15/20], Loss: 0.6098, Training Accuracy: 75.00%\n",
            "Validation Loss: 0.6698, Validation Accuracy: 73.56%\n",
            "Epoch [16/20] - Current Learning Rate: 0.0001\n",
            "Epoch [16/20], Loss: 0.6044, Training Accuracy: 74.85%\n",
            "Validation Loss: 0.6728, Validation Accuracy: 74.52%\n",
            "Epoch [17/20] - Current Learning Rate: 0.0001\n",
            "Epoch [17/20], Loss: 0.6149, Training Accuracy: 74.96%\n",
            "Validation Loss: 0.6679, Validation Accuracy: 74.14%\n",
            "Epoch [18/20] - Current Learning Rate: 0.0001\n",
            "Epoch [18/20], Loss: 0.6066, Training Accuracy: 75.56%\n",
            "Validation Loss: 0.6543, Validation Accuracy: 74.14%\n",
            "Epoch [19/20] - Current Learning Rate: 0.0001\n",
            "Epoch [19/20], Loss: 0.5960, Training Accuracy: 74.75%\n",
            "Validation Loss: 0.6545, Validation Accuracy: 74.90%\n",
            "Epoch [20/20] - Current Learning Rate: 0.0001\n",
            "Epoch [20/20], Loss: 0.5968, Training Accuracy: 74.90%\n",
            "Validation Loss: 0.6596, Validation Accuracy: 74.33%\n",
            "Training finished.\n",
            "Model weights saved to /content/drive/MyDrive/datasets/Inception_1layer_nodropout_weights.pth\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
        "\n",
        "# Set up learning rate scheduler with patience = 2\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Stop training if no improvement after 3 epochs\n",
        "early_stopping_counter = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Print the current learning rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Current Learning Rate: {param_group['lr']}\")\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training phase\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Print training loss and accuracy\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Move images and labels to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    # Calculate validation loss and accuracy\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Check for learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0  # Reset the early stopping counter\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f'Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print('Training finished.')\n",
        "\n",
        "# Define the path where you want to save the model weights in Google Drive\n",
        "save_path = '/content/drive/MyDrive/datasets/Inception_1layer_nodropout_weights.pth'\n",
        "\n",
        "# After training is finished, save the model weights\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f'Model weights saved to {save_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Additional Training\n",
        "\n",
        "Configure training parameters:\n",
        "- Loss function: Cross Entropy\n",
        "- Optimizer: Adam\n",
        "- Learning rate scheduling\n",
        "- Early stopping\n",
        "- Monitor training and validation accuracy to watch for overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92L7Jc2AiOE-",
        "outputId": "abe72ffe-6bf0-4812-de1e-8e2c33b5c284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20] - Current Learning Rate: 0.0001\n",
            "Epoch [1/20], Loss: 0.5966, Training Accuracy: 75.13%\n",
            "Validation Loss: 0.6684, Validation Accuracy: 73.56%\n",
            "Epoch [2/20] - Current Learning Rate: 0.0001\n",
            "Epoch [2/20], Loss: 0.6001, Training Accuracy: 75.29%\n",
            "Validation Loss: 0.6793, Validation Accuracy: 74.14%\n",
            "Epoch [3/20] - Current Learning Rate: 0.0001\n",
            "Epoch [3/20], Loss: 0.5809, Training Accuracy: 75.88%\n",
            "Validation Loss: 0.6549, Validation Accuracy: 74.14%\n",
            "Epoch [4/20] - Current Learning Rate: 0.0001\n",
            "Epoch [4/20], Loss: 0.5912, Training Accuracy: 74.90%\n",
            "Validation Loss: 0.6532, Validation Accuracy: 74.71%\n",
            "Epoch [5/20] - Current Learning Rate: 0.0001\n",
            "Epoch [5/20], Loss: 0.5827, Training Accuracy: 75.98%\n",
            "Validation Loss: 0.6489, Validation Accuracy: 74.33%\n",
            "Epoch [6/20] - Current Learning Rate: 0.0001\n",
            "Epoch [6/20], Loss: 0.5830, Training Accuracy: 75.54%\n",
            "Validation Loss: 0.6443, Validation Accuracy: 74.71%\n",
            "Epoch [7/20] - Current Learning Rate: 0.0001\n",
            "Epoch [7/20], Loss: 0.5904, Training Accuracy: 75.17%\n",
            "Validation Loss: 0.6299, Validation Accuracy: 74.90%\n",
            "Epoch [8/20] - Current Learning Rate: 0.0001\n",
            "Epoch [8/20], Loss: 0.5877, Training Accuracy: 75.58%\n",
            "Validation Loss: 0.6479, Validation Accuracy: 74.52%\n",
            "Epoch [9/20] - Current Learning Rate: 0.0001\n",
            "Epoch [9/20], Loss: 0.5967, Training Accuracy: 75.31%\n",
            "Validation Loss: 0.6569, Validation Accuracy: 74.33%\n",
            "Epoch [10/20] - Current Learning Rate: 0.0001\n",
            "Epoch [10/20], Loss: 0.5783, Training Accuracy: 75.67%\n",
            "Validation Loss: 0.6388, Validation Accuracy: 74.33%\n",
            "Early stopping at epoch 10\n",
            "Training finished.\n",
            "Model weights saved to /content/drive/MyDrive/datasets/Inception_1layer_nodropout_weights_continued.pth\n"
          ]
        }
      ],
      "source": [
        "# Set up loss function and optimizer (you don't need to change this part)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
        "\n",
        "# Set up learning rate scheduler with patience = 2\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Stop training if no improvement after 3 epochs\n",
        "early_stopping_counter = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Continue Training Loop for additional epochs\n",
        "additional_epochs = 20  # Set how many additional epochs you want to train for\n",
        "\n",
        "for epoch in range(additional_epochs):\n",
        "    # Print the current learning rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(f\"Epoch [{epoch+1}/{additional_epochs}] - Current Learning Rate: {param_group['lr']}\")\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training phase\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Print training loss and accuracy\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f'Epoch [{epoch+1}/{additional_epochs}], Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Move images and labels to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    # Calculate validation loss and accuracy\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Check for learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0  # Reset the early stopping counter\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f'Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print('Training finished.')\n",
        "\n",
        "# Save the model weights after additional training\n",
        "save_path = '/content/drive/MyDrive/datasets/Inception_1layer_nodropout_weights_continued.pth'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f'Model weights saved to {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTcn6whtiOXr",
        "outputId": "5f36f2c6-d992-4034-8b6c-43e83b00ae9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 80.13%\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Function to evaluate the model on the test dataset\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            # Move images and labels to the device (GPU)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Example usage after training\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHr3EAmnvH3i"
      },
      "source": [
        "## 7. Update Training Configuration and Continue\n",
        "- Unfreeze another layer and continue training for 20 more epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT18x1dXvJ9x"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the last two Inception blocks (Mixed_7b and Mixed_7c)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers first\n",
        "\n",
        "# Unfreeze the last two inception blocks and the fully connected layer\n",
        "for name, param in model.named_parameters():\n",
        "    if 'Mixed_7b' in name or 'Mixed_7c' in name or 'fc' in name:\n",
        "        param.requires_grad = True  # Unfreeze the specified layers\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEdaXMH-vRrr",
        "outputId": "7fd38254-2792-4e63-92aa-ba94e2e98d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20] - Current Learning Rate: 0.0001\n",
            "Epoch [1/20], Loss: 0.5169, Training Accuracy: 77.80%\n",
            "Validation Loss: 0.4560, Validation Accuracy: 82.18%\n",
            "Epoch [2/20] - Current Learning Rate: 0.0001\n",
            "Epoch [2/20], Loss: 0.4261, Training Accuracy: 81.54%\n",
            "Validation Loss: 0.4034, Validation Accuracy: 82.57%\n",
            "Epoch [3/20] - Current Learning Rate: 0.0001\n",
            "Epoch [3/20], Loss: 0.3622, Training Accuracy: 84.43%\n",
            "Validation Loss: 0.3588, Validation Accuracy: 85.82%\n",
            "Epoch [4/20] - Current Learning Rate: 0.0001\n",
            "Epoch [4/20], Loss: 0.3247, Training Accuracy: 85.99%\n",
            "Validation Loss: 0.3289, Validation Accuracy: 88.89%\n",
            "Epoch [5/20] - Current Learning Rate: 0.0001\n",
            "Epoch [5/20], Loss: 0.2704, Training Accuracy: 88.57%\n",
            "Validation Loss: 0.3032, Validation Accuracy: 88.70%\n",
            "Epoch [6/20] - Current Learning Rate: 0.0001\n",
            "Epoch [6/20], Loss: 0.2303, Training Accuracy: 90.38%\n",
            "Validation Loss: 0.3372, Validation Accuracy: 89.08%\n",
            "Epoch [7/20] - Current Learning Rate: 0.0001\n",
            "Epoch [7/20], Loss: 0.2022, Training Accuracy: 91.99%\n",
            "Validation Loss: 0.2190, Validation Accuracy: 90.80%\n",
            "Epoch [8/20] - Current Learning Rate: 0.0001\n",
            "Epoch [8/20], Loss: 0.1891, Training Accuracy: 92.24%\n",
            "Validation Loss: 0.2463, Validation Accuracy: 90.61%\n",
            "Epoch [9/20] - Current Learning Rate: 0.0001\n",
            "Epoch [9/20], Loss: 0.1691, Training Accuracy: 93.35%\n",
            "Validation Loss: 0.2152, Validation Accuracy: 93.87%\n",
            "Epoch [10/20] - Current Learning Rate: 0.0001\n",
            "Epoch [10/20], Loss: 0.1471, Training Accuracy: 94.46%\n",
            "Validation Loss: 0.1886, Validation Accuracy: 94.25%\n",
            "Epoch [11/20] - Current Learning Rate: 0.0001\n",
            "Epoch [11/20], Loss: 0.1307, Training Accuracy: 95.03%\n",
            "Validation Loss: 0.1629, Validation Accuracy: 94.64%\n",
            "Epoch [12/20] - Current Learning Rate: 0.0001\n",
            "Epoch [12/20], Loss: 0.1091, Training Accuracy: 95.84%\n",
            "Validation Loss: 0.1745, Validation Accuracy: 92.53%\n",
            "Epoch [13/20] - Current Learning Rate: 0.0001\n",
            "Epoch [13/20], Loss: 0.1000, Training Accuracy: 96.03%\n",
            "Validation Loss: 0.2288, Validation Accuracy: 93.10%\n",
            "Epoch [14/20] - Current Learning Rate: 0.0001\n",
            "Epoch [14/20], Loss: 0.0978, Training Accuracy: 96.38%\n",
            "Validation Loss: 0.1921, Validation Accuracy: 94.06%\n",
            "Early stopping at epoch 14\n",
            "Training finished.\n",
            "Model weights saved to /content/drive/MyDrive/datasets/Inception_1layer_nodropout_weights_3.pth\n"
          ]
        }
      ],
      "source": [
        "# Update the optimizer to include the trainable parameters\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Print the current learning rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Current Learning Rate: {param_group['lr']}\")\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training phase\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Print training loss and accuracy\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Move images and labels to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    # Calculate validation loss and accuracy\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Check for learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0  # Reset the early stopping counter\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f'Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print('Training finished.')\n",
        "\n",
        "# Define the path where you want to save the model weights in Google Drive\n",
        "save_path = '/content/drive/MyDrive/datasets/Inception_1layer_nodropout_weights_3.pth'\n",
        "\n",
        "# After training is finished, save the model weights\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f'Model weights saved to {save_path}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
