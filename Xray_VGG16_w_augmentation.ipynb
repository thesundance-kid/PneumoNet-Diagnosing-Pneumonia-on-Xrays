{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s69SdeCIuiwA",
        "outputId": "a9899604-299d-42b4-abf0-2d3808870d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/datasets/train /content/train"
      ],
      "metadata": {
        "id": "pjwYNJ-K2Flx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data Augmentation**\n"
      ],
      "metadata": {
        "id": "LqNm29Zxu8V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initial transform: Resize and convert to tensor only\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Ensure all images are the same size\n",
        "    transforms.ToTensor()  # Convert images to tensor format\n",
        "])\n",
        "\n",
        "# Load dataset with simple transform (no normalization)\n",
        "train_dataset_simple = datasets.ImageFolder(root='/content/train', transform=simple_transform)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "train_loader_simple = DataLoader(train_dataset_simple, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "def compute_mean_std(loader):\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    total_images_count = 0\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0)  # Number of images in the batch\n",
        "        images = images.view(batch_samples, images.size(1), -1)  # Flatten the images\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        total_images_count += batch_samples\n",
        "\n",
        "    mean /= total_images_count\n",
        "    std /= total_images_count\n",
        "    return mean, std\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean, std = compute_mean_std(train_loader_simple)\n",
        "\n",
        "print(f\"Mean: {mean}, Std: {std}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Owoht2-vEjM",
        "outputId": "c762f968-122d-4f48-ebf2-e4482518a41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: tensor([0.4823, 0.4823, 0.4823]), Std: tensor([0.2216, 0.2216, 0.2216])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the computed mean and std values and perform data augmentation on train dataset\n",
        "train_transform = transforms.Compose([\n",
        "       transforms.RandomRotation(10),  # Small rotations for generalization\n",
        "       transforms.RandomAffine(degrees=0, translate=(0.03, 0.03)),  # Small translations\n",
        "       transforms.RandomResizedCrop(224, scale=(0.95, 1.05)),  # Slight zoom\n",
        "       transforms.ToTensor(),  # Convert to tensor\n",
        "       transforms.Normalize(mean=mean.tolist(), std=std.tolist())  # Use computed mean and std\n",
        "   ])\n",
        "\n",
        "#Only resize and normalize test dataset\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())  # Same normalization for test set\n",
        "])\n",
        "\n",
        "!cp -r /content/drive/MyDrive/datasets/test /content/test\n",
        "\n",
        "# Load dataset (assumes dataset is in a directory with subfolders for each class)\n",
        "train_dataset = datasets.ImageFolder(root='/content/train', transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder(root='/content/test', transform=test_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers= 2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Check class to index mapping\n",
        "print(train_dataset.class_to_idx)  # {'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wEINhPqwH-E",
        "outputId": "9050382e-68ec-4a7b-b7a4-6bf29a148db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a_normal_xrays': 0, 'bacterial pneumonia': 1, 'viral pneumonia': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data loaded with augmentation methods. Load/define Vgg16 model for transfer learning**"
      ],
      "metadata": {
        "id": "EubSivwG4bbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Load a pretrained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Modify the classifier (for your specific number of classes, e.g., 2 classes for binary classification)\n",
        "model.classifier[6] = nn.Linear(4096, 3)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Train for 10 epochs\n",
        "num_epochs = 10\n",
        "\n"
      ],
      "metadata": {
        "id": "MrFiPZQB4Na1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Training**"
      ],
      "metadata": {
        "id": "EzCUcrXf49A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n",
        "\n",
        "    # Step the scheduler based on the average loss\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    # Log the current learning rate\n",
        "    current_lr = scheduler.get_last_lr()\n",
        "    print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "\n",
        "print('Training finished.')\n",
        "\n",
        "# Save the final model state after all epochs\n",
        "torch.save(model.state_dict(), 'VGG16_on_xray_weights_data_augmentation1.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/VGG16_on_xray_weights_data_augmentation1.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b72XvJff4_R9",
        "outputId": "e3ea4cb4-2100-4ed7-c8ab-c6b6b2c6b002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7997261817835591\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [2/10], Loss: 0.7005845346699463\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [3/10], Loss: 0.6579501560312108\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [4/10], Loss: 0.5694618964670626\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [5/10], Loss: 0.5455121008530717\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [6/10], Loss: 0.5089866733806996\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [7/10], Loss: 0.5174218904387\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [8/10], Loss: 0.49219228564961554\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [9/10], Loss: 0.49413634544135604\n",
            "Current Learning Rate: [0.001]\n",
            "Epoch [10/10], Loss: 0.4880158719292448\n",
            "Current Learning Rate: [0.001]\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for an additional 20 epochs at .0001 lr\n",
        "\n",
        "# Load the saved model state to resume training\n",
        "model.load_state_dict(torch.load('VGG16_on_xray_weights_data_augmentation1.pth', weights_only = True))\n",
        "\n",
        "# Freeze all layers except the classifier (if not already done)\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Continue training for another 20 epochs\n",
        "num_epochs = 20  # Adjust the number of additional epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n",
        "\n",
        "    # Step the scheduler after each epoch, based on the average loss\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "print('Additional training finished.')\n",
        "\n",
        "# Save the final model state after all epochs\n",
        "torch.save(model.state_dict(), 'VGG16_on_xray_weights_data_augmentation1.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/VGG16_on_xray_weights_data_augmentation1.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhsaOUxCCmzE",
        "outputId": "8556e52c-5ab8-446d-c4b3-37f5e733bad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.39685526188531534\n",
            "Epoch [2/20], Loss: 0.368242229100751\n",
            "Epoch [3/20], Loss: 0.3466511806644545\n",
            "Epoch [4/20], Loss: 0.35579090263763086\n",
            "Epoch [5/20], Loss: 0.3460860557717048\n",
            "Epoch [6/20], Loss: 0.3337754714159878\n",
            "Epoch [7/20], Loss: 0.32607292050232917\n",
            "Epoch [8/20], Loss: 0.32333679853772823\n",
            "Epoch [9/20], Loss: 0.31555928134479405\n",
            "Epoch [10/20], Loss: 0.3192964631355613\n",
            "Epoch [11/20], Loss: 0.31350377243172173\n",
            "Epoch [12/20], Loss: 0.30251902666377145\n",
            "Epoch [13/20], Loss: 0.293893978456778\n",
            "Epoch [14/20], Loss: 0.2960901893446782\n",
            "Epoch [15/20], Loss: 0.29379256764438255\n",
            "Epoch [16/20], Loss: 0.28939124425313223\n",
            "Epoch [17/20], Loss: 0.28701969094437324\n",
            "Epoch [18/20], Loss: 0.27816906740511854\n",
            "Epoch [19/20], Loss: 0.2758285485527998\n",
            "Epoch [20/20], Loss: 0.2642974432534967\n",
            "Additional training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Function to evaluate the model on the test dataset\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            # Move images and labels to the device (GPU)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Example usage after training\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV2BSmFNASKp",
        "outputId": "2665fe7e-4d48-4b44-a1e5-98b25b051d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 82.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Additional Training**"
      ],
      "metadata": {
        "id": "TOE_oGvt6kY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Step 1: Initialize the model architecture with the correct number of classes\n",
        "num_classes = 3  # Replace with the actual number of classes in your dataset\n",
        "model = models.vgg16(pretrained=False, num_classes=num_classes)\n",
        "\n",
        "# Step 2: Load the saved weights from previous training session\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/VGG16_on_xray_weights_data_augmentation1.pth'))\n",
        "\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Move the model to the GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Define same loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # loss had not increased twice consecutively, so setting lr as same value\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2AkPDYK6jfN",
        "outputId": "d259d302-feaa-47e8-bb63-c0f455216e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-6-d0ea4be60162>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/VGG16_on_xray_weights_data_augmentation1.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training for another 20 epochs\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Move images and labels to the device (GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n",
        "\n",
        "    # Step the scheduler after each epoch, based on the average loss\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "print('Additional training finished.')\n",
        "\n",
        "# Save the final model state after all epochs, save in different file in case\n",
        "torch.save(model.state_dict(), 'VGG16_on_xray_weights_data_augmentation2.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/VGG16_on_xray_weights_data_augmentation2.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMIUivVa8Kh-",
        "outputId": "010c1343-26bf-4078-ac0b-49127a136dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2731098582042507\n",
            "Epoch [2/20], Loss: 0.2554500456328041\n",
            "Epoch [3/20], Loss: 0.24734452485672534\n",
            "Epoch [4/20], Loss: 0.25171911821592075\n",
            "Epoch [5/20], Loss: 0.24728679871815115\n",
            "Epoch [6/20], Loss: 0.22475284792802816\n",
            "Epoch [7/20], Loss: 0.23835025471778004\n",
            "Epoch [8/20], Loss: 0.21908789251479635\n",
            "Epoch [9/20], Loss: 0.22775914949690637\n",
            "Epoch [10/20], Loss: 0.20906987347485828\n",
            "Epoch [11/20], Loss: 0.21901801494778667\n",
            "Epoch [12/20], Loss: 0.2016317397813124\n",
            "Epoch [13/20], Loss: 0.19505999561833456\n",
            "Epoch [14/20], Loss: 0.2059084623137866\n",
            "Epoch [15/20], Loss: 0.19688985032812217\n",
            "Epoch [16/20], Loss: 0.18734390441716814\n",
            "Epoch [17/20], Loss: 0.19018937775328115\n",
            "Epoch [18/20], Loss: 0.18963840091886697\n",
            "Epoch [19/20], Loss: 0.171672854126871\n",
            "Epoch [20/20], Loss: 0.16703784030410776\n",
            "Additional training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate once more on test set\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Function to evaluate the model on the test dataset\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            # Move images and labels to the device (GPU)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Example usage after training\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc6v13eIHvKv",
        "outputId": "4efad516-ed48-4a44-f6f8-ec904f7f7e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not seeing further improvement in test accuracy. Potentially overfitting training set\n"
      ],
      "metadata": {
        "id": "rizwA9jRIOsc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gjGPujXnIYM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}